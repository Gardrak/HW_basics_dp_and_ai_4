Shallow: Params=60362, Test Acc=71.91%, Train Time=82.75s
  conv1.weight: grad=0.079428
  conv1.bias: grad=0.169473
  conv2.weight: grad=0.008225
  conv2.bias: grad=0.054813
  fc.weight: grad=0.004515
  fc.bias: grad=0.036473
Medium: Params=398666, Test Acc=75.15%, Train Time=90.34s
  conv1.weight: grad=0.077812
  conv1.bias: grad=0.160413
  conv2.weight: grad=0.006082
  conv2.bias: grad=0.045623
  conv3.weight: grad=0.002575
  conv3.bias: grad=0.013205
  conv4.weight: grad=0.000979
  conv4.bias: grad=0.004733
  fc.weight: grad=0.006648
  fc.bias: grad=0.019036
Deep: Params=288298, Test Acc=72.40%, Train Time=100.33s
  conv1.weight: grad=0.078609
  conv1.bias: grad=0.182562
  conv2.weight: grad=0.006884
  conv2.bias: grad=0.109522
  conv3.weight: grad=0.004036
  conv3.bias: grad=0.045802
  conv4.weight: grad=0.001274
  conv4.bias: grad=0.017325
  conv5.weight: grad=0.000520
  conv5.bias: grad=0.005733
  conv6.weight: grad=0.000205
  conv6.bias: grad=0.001694
  fc.weight: grad=0.016873
  fc.bias: grad=0.032133
ResNet: Params=696618, Test Acc=82.17%, Train Time=203.93s
  conv1.weight: grad=0.031380
  bn1.weight: grad=0.010225
  bn1.bias: grad=0.009708
  layer1.0.conv1.weight: grad=0.002108
  layer1.0.bn1.weight: grad=0.005704
  layer1.0.bn1.bias: grad=0.005229
  layer1.0.conv2.weight: grad=0.002273
  layer1.0.bn2.weight: grad=0.004839
  layer1.0.bn2.bias: grad=0.004987
  layer1.1.conv1.weight: grad=0.001903
  layer1.1.bn1.weight: grad=0.004594
  layer1.1.bn1.bias: grad=0.003635
  layer1.1.conv2.weight: grad=0.001752
  layer1.1.bn2.weight: grad=0.005533
  layer1.1.bn2.bias: grad=0.003079
  layer2.0.conv1.weight: grad=0.001303
  layer2.0.bn1.weight: grad=0.004429
  layer2.0.bn1.bias: grad=0.003809
  layer2.0.conv2.weight: grad=0.001129
  layer2.0.bn2.weight: grad=0.005858
  layer2.0.bn2.bias: grad=0.004385
  layer2.0.shortcut.0.weight: grad=0.005197
  layer2.0.shortcut.1.weight: grad=0.006153
  layer2.0.shortcut.1.bias: grad=0.004385
  layer2.1.conv1.weight: grad=0.001015
  layer2.1.bn1.weight: grad=0.004845
  layer2.1.bn1.bias: grad=0.004282
  layer2.1.conv2.weight: grad=0.000864
  layer2.1.bn2.weight: grad=0.003656
  layer2.1.bn2.bias: grad=0.003026
  layer3.0.conv1.weight: grad=0.000701
  layer3.0.bn1.weight: grad=0.003601
  layer3.0.bn1.bias: grad=0.003240
  layer3.0.conv2.weight: grad=0.000355
  layer3.0.bn2.weight: grad=0.003630
  layer3.0.bn2.bias: grad=0.003977
  layer3.0.shortcut.0.weight: grad=0.001812
  layer3.0.shortcut.1.weight: grad=0.003553
  layer3.0.shortcut.1.bias: grad=0.003977
  layer3.1.conv1.weight: grad=0.000332
  layer3.1.bn1.weight: grad=0.004472
  layer3.1.bn1.bias: grad=0.003307
  layer3.1.conv2.weight: grad=0.000168
  layer3.1.bn2.weight: grad=0.003047
  layer3.1.bn2.bias: grad=0.004906
  fc.weight: grad=0.018540
  fc.bias: grad=0.021755
